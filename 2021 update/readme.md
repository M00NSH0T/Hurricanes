# Check Out the Jupyter Notebooks in this Directory

The notebooks are numbered in the order you should view them in. However, the first notebook is quite long as it contains everything from preprocessing to visualizing predicted hurricane tracks for the basline model, so you may need to click the link to reload it a couple times to get it to display in GitHub. 

The first notebook walks through how to tie into the AWS GEOS16 satellite data, along with historic hurricane track data, to create a set of several thousand views of several hundred storms from space. I combine satellite imagery using wavelengths outside the visible spectrum to ensure we see detail at night as well as during the day, and superimpose a slightly transparent view of these storms over a terrain map of the world, so our images also show coastline and other terrain detail usually obscured by cloud cover, and all but invisible at night. Here's a sample:

![image](https://github.com/M00NSH0T/Hurricanes/blob/master/2021%20update/storm_centered/centered_2017152N14262_20171523.png)

Next, I build a model based on the wide and deep architecture which ties together the image data, along with numerical and categorical data via three separate trees, which then come together to generate forecasts of the storm's change in latitude / longitude 48 hours in the future. The wide network also takes in bucketized, engineered versions of certain continuous variables, as well as feature crosses of bucketized latitude and longitude coordinates via Tensorflow's feature_column class. An autogenerated representation of the baseline model is as follows:

![image](https://github.com/M00NSH0T/Hurricanes/blob/master/2021%20update/baseline_model.png)

The second and third notebooks attempt to improve upon the model of the first by adding a CNN, and then using Transfer Learning with MobileNetV2. Unfortunately, the performance improvements, at least to validation error, were negligible if present at all. 

The third notebook attemps to bring in more data via the creation and use of composite full disk views of what the GOES16 satellite views across three of its bands. All three of these bands are well outside the visible spectrum, but they are used as the RGB color channels of a synthetic generated view. My thinking here was that by using channels outside the visible light spectrum, the model would see negligible differences between daytime and nighttime, which would aid in training, and by combining these channels, it would be able to discern more detail than with one band alone. Certainly looking at these images with the human eye, it's easy to see more of the storm pattens than by looking at any single band's image alone. Here's an example of a composite image:

![image](https://github.com/M00NSH0T/Hurricanes/blob/master/2021%20update/full_disks/fd20171069.png)

My next goal here is to revisit this general approach using alternative data sources, such as some of the derived variables output by GOES16/17, as well as the realtime composite data provided by forecasting models available in the cloud to generate similar views of the present storm, and then augment the training data with similar data from the Reanalysis 2 dataset.

If I can bring in enough historic data, I will also explore the use of LSTM / GRU versions of the image processing branches of the model.

